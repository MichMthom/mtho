import pandas as pd
from aif360.algorithms.preprocessing import Reweighing
from aif360.datasets import BinaryLabelDataset

# Load the COMPAS dataset
compas_df = pd.read_csv('compas.csv')

# Convert the dataset to a BinaryLabelDataset
dataset = BinaryLabelDataset(df=compas_df, label_names=['two_year_recid'], protected_attribute_names=['race'], favorable_label=0, unfavorable_label=1)

# Implement Reweighing
rw = Reweighing(unprivileged_groups=[{'race': 1}], privileged_groups=[{'race': 0}])
transformed_dataset = rw.fit_transform(dataset)Short Answer Questions
Q1: Algorithmic Bias
Algorithmic bias refers to the unfair or discriminatory outcomes produced by AI systems due to biases in the data, algorithms, or design. Two examples of algorithmic bias include:

- *Biased hiring tools*: AI-powered hiring tools that penalize female candidates or candidates from underrepresented groups.
- *Facial recognition systems*: Facial recognition systems that misidentify minorities at higher rates, leading to potential wrongful arrests or privacy violations.

Q2: Transparency and Explainability
Transparency in AI refers to the ability to understand how AI systems work and make decisions. Explainability, on the other hand, refers to the ability to understand why a specific decision was made. Both transparency and explainability are important because they enable trust, accountability, and fairness in AI systems.

Q3: GDPR Impact on AI Development
The General Data Protection Regulation (GDPR) impacts AI development in the EU by requiring organizations to obtain explicit consent from users for data collection and processing. GDPR also emphasizes transparency, user control, and accountability, which can influence the design and deployment of AI systems.

Ethical Principles Matching
A) Justice: Fair distribution of AI benefits and risks.
B) Non-maleficence: Ensuring AI does not harm individuals or society.
C) Autonomy: Respecting usersâ€™ right to control their data and decisions.
D) Sustainability: Designing AI to be environmentally friendly.

Case Study Analysis
1. *Biased Hiring Tool*

- *Source of bias*: The training data used to develop the hiring tool was biased towards male candidates, resulting in the tool penalizing female candidates.
- *Fixes*:
    1. *Data preprocessing*: Ensure the training data is diverse and representative of all candidates.
    2. *Model design*: Use fairness metrics to evaluate and mitigate bias in the model.
    3. *Human oversight*: Implement human oversight to review and correct AI-driven decisions.
- *Metrics to evaluate fairness*: Use metrics such as demographic parity, equal opportunity, and predictive rate parity to evaluate fairness post-correction.

Case 2: Facial Recognition in Policing
- *Ethical risks*: Wrongful arrests, privacy violations, and perpetuation of existing biases.
- *Policies for responsible deployment*:
    1. *Transparency*: Ensure transparency about the use of facial recognition technology and its limitations.
    2. *Human oversight*: Implement human oversight to review and correct AI-driven decisions.
    3. *Regular audits*: Conduct regular audits to ensure the technology is fair and unbiased.


Part 3: Practical Audit
Example Report
The COMPAS dataset audit revealed significant racial bias in the risk scores, with African American defendants being more likely to be misclassified as high-risk. To mitigate this bias, we implemented Reweighing, which reduced the disparity in false positive rates. Our findings highlight the importance of auditing AI systems for bias and fairness.

*Example Findings:*

- Disparity in false positive rates: 20% higher for African American defendants compared to Caucasian defendants.
- After implementing Reweighing: Disparity reduced to 5%.

Example Code
import pandas as pd
from aif360.algorithms.preprocessing import Reweighing
from aif360.datasets import BinaryLabelDataset

Load the COMPAS dataset
compas_df = pd.read_csv('compas.csv')

Convert the dataset to a BinaryLabelDataset
dataset = BinaryLabelDataset(df=compas_df, label_names=['two_year_recid'], protected_attribute_names=['race'], favorable_label=0, unfavorable_label=1)

Implement Reweighing
rw = Reweighing(unprivileged_groups=[{'race': 1}], privileged_groups=[{'race': 0}])
transformed_dataset = rw.fit_transform(dataset)
Part 4: Ethical Reflection
Example Reflection
When developing an AI-powered healthcare system, I will prioritize transparency, accountability, and fairness. This includes providing clear explanations of AI-driven decisions, implementing mechanisms for accountability and oversight, and ensuring that the system is fair and unbiased.

*Example Commitments:*

- Transparency: Provide clear explanations of AI-driven decisions and their underlying logic.
- Accountability: Implement mechanisms for accountability and oversight to ensure that the AI system is fair and unbiased.
- Fairness: Ensure that the AI system is fair and unbiased, and that it does not perpetuate existing biases or discriminatory outcomes.

Bonus Task: Policy Proposal
Example Policy Proposal
*Ethical AI Use in Healthcare Policy*

*Purpose:* Ensure that AI systems in healthcare are fair, transparent, and accountable.

*Scope:* All AI systems used in healthcare, including diagnostic and treatment recommendation systems.

*Key Provisions:*

- Patient consent protocols: Obtain explicit consent from patients for data collection and processing.
- Bias mitigation strategies: Implement fairness metrics and bias mitigation techniques to reduce disparities in outcomes.
- Transparency requirements: Provide clear explanations of AI-driven decisions and their underlying logic.

*Implementation:*

- Conduct regular audits to ensure compliance with policy provisions.
- Provide training and education for healthcare professionals on AI ethics and responsible AI use.

By implementing this policy, we can ensure that AI systems in healthcare are used responsibly and prioritize patient safety and well-being.
